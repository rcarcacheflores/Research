---
layout: post
title: Modelling obesity with decision trees (PY)
date: 2020-10-7 14:47:49 +0300
description: Using a decision tree regression to categorize obesity prevalence in 112 countries
img: post6.jpg
tags: [Research, Machine Learning basics, Python, Decision trees]
---

My last post was a bit lengthy so I will try to keep things short and (hopefully) simple. In this post we will use the GFSI data and perform a decision tree regression. **The goal is to verify which variables are the most important in determining the countries with the highest obesity prevalence**. This is the first time I use any ML algorithm in Python, so let's see what happens!

## Part 1: Preparing our data and performing the tree regression

Decision tree regressions allow us to use decision trees to model numeric data. According to [Langtz (2013):](https://edu.kpfu.ru/pluginfile.php/278552/mod_resource/content/1/MachineLearningR__Brett_Lantz.pdf) 
>"decision tree learners build a model in the form
>of a tree structure. The model itself comprises a series of logical decisions, similar to
>a flowchart, with decision nodes that indicate a decision to be made on an attribute.
>These split into branches that indicate the decision's choices."

For a more illustrative and simple application of a decision tree regression, visit this [excellent post](https://medium.com/swlh/making-data-trees-in-python-3a3ceb050cfd). I am still a novice when it comes to using Machine Learning algorithms. However, I find it much easier to perform the models and then interpret the output. Let's start!


```python
#first we will load our GFSI database:
import pandas as pd
d1 = pd.read_csv("gsfi_logit.csv")
#for this model we do not need the country variable so we can drop it
#we specify the axis to say that it is a column and not a row
d1 = d1.drop('country', axis=1)
#let's confirm if it worked:
d1.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: middle;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: middle;">
      <th></th>
      <th>obesity</th>
      <th>PPC</th>
      <th>KCAL</th>
      <th>div</th>
      <th>poverty</th>
      <th>consumption</th>
      <th>popdens</th>
      <th>OECD</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>23.5</td>
      <td>13760.92</td>
      <td>3229.4</td>
      <td>42.0</td>
      <td>7.720</td>
      <td>43.090000</td>
      <td>16.416173</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>9.1</td>
      <td>7340.70</td>
      <td>2200.8</td>
      <td>39.2</td>
      <td>54.516</td>
      <td>50.450000</td>
      <td>21.607079</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>24.7</td>
      <td>21893.08</td>
      <td>3017.4</td>
      <td>66.0</td>
      <td>3.630</td>
      <td>33.170000</td>
      <td>15.703889</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>27.0</td>
      <td>44167.94</td>
      <td>3255.8</td>
      <td>74.8</td>
      <td>0.000</td>
      <td>10.050245</td>
      <td>3.057785</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>17.4</td>
      <td>45358.62</td>
      <td>3797.4</td>
      <td>73.0</td>
      <td>0.000</td>
      <td>9.937617</td>
      <td>103.756755</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



Now that we removed this column, we have to create two subsets for our tree regression. We will call "x" the subset which contains all of the regressors. Thus, "y" will be the obesity prevalence variable, which is the one we are trying to model here. Then, we will import the decision tree regressor from [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html):


```python
#creating the x subset:
x = d1.drop('obesity', axis=1)
y = d1['obesity']
#now we import the corresponding function
#make sure to have sci-kit installed before doing so:
from sklearn.tree import DecisionTreeRegressor  
#create a regressor object with default settings:
regressor = DecisionTreeRegressor(random_state = 0)  
# fit the regressor with the x and y data:
model = regressor.fit(x, y)
```

## Part 2: Interpeting the model output

So far we have generated an object called "model" from our decision tree regression. We must remember that we are not using this model to generate predictions. We simply want to make a decision tree to see how this ML algorithm classifies our regressors. Before doing so, we can inspect some of the attributes our model has. The simplest way is to type *model?* and this will generate a length description of the features and attributes of this model. For now we will focus on two attributes:


```python
#first we check the number of features in our model:
model.max_features_
```




    7



This matches to the seven regressors we used in our subset "x".


```python
#next we will call for the most important features of this model:
model.feature_importances_
```




    array([0.58775826, 0.1246276 , 0.07449675, 0.068188  , 0.02930249,
           0.06683283, 0.04879407])



The higher the value in the array, the more important the feature. If we recall the order of our features, the tree regression places the most importance on the GDP per capita variable, followed by KCAL and div. This is a similar result to what we obtained in the previous post. The least important variable seemes to be the average household expenditure on food consumption.

## Part 3: Visualizing our tree

We just performed a simple exploration of the decision tree regression model. In general terms, the model has classified each regressor by the Mean Square Error criteria. We have seen that GDP per capita is the most important regressor we have for modelling obesity prevalence. Now we can [visualize our results](https://www.geeksforgeeks.org/python-decision-tree-regression-using-sklearn/) by generating a decision tree with our output. This is a slightly complicated procedure but let's start with the Python syntax:


```python
#first we will import the library we want from sklearn:
from sklearn.tree import export_graphviz  
  
#now we export the regression tree as a tree.dot file
#note that we should specify the feature names as the x columns:
export_graphviz(regressor, out_file ='tree1.dot', 
                feature_names=x.columns) 
```

So where is the tree? If we check our working directory, *export_graphviz* has generated a file called "tree1.dot". From here we need to do the following:

1. Open the "tree1.dot" file using Word or another compatible text editor.
2. Check the text code to make sure the correct features were added.
3. Copy and paste all of the code into [WebGraphviz](http://www.webgraphviz.com).
4. Use this site to generate this tree and save the output as a PDF or html. 

If everything went well (fingers crossed) you should have something like this:

![tree 1]({{site.baseurl}}/assets/img/post6_p1.png)



**That is a really big tree!** I had to zoom out to 9% on the PDF to visualize the entire structure. Notice how there is a root at the top and from there we have two decision nodes going in opposite directions. In general terms, the left side of the tree shows the countries with the lowest obesity prevalence. The countries with the highest obesity prevalence are in the right side of tree. 

This is impossible to see from our image so let's zoom in and focus on the top of the tree. In order to this we can open the "tree.dot" file and trim the tree manually before creating a new diagram. I'm sure there is a more efficient way of doing this, but this is the first time I do this in Python. The output should look like this after a lot of re-arranging:


![tree 2]({{site.baseurl}}/assets/img/post6_p2.png)


Let's draw some observations from the top of our tree diagram:

1. At the top of the tree we have our root, shown as a double circle node. **The root of our tree is GDP per capita!** Countries with the highest obesity levels have GDP per capita of 10,319 USD or more. Notice the value at the bottom of the node. That is the mean obesity prevalence we modelled in our logit. 
2. **Let's go left**. The countries with below average obesity prevalence have poverty levels below 36%. The sample figure shows this constitutes 50 countries. From there, we can see different combinations of variables determining the leafs of the trees.5
3. **Now we shift to the right, to the most obese countries**. The decision node here is population density. The most obese countries have a population density less than 129 per square kilometer. From there, our tree seems to break off into middle to high-income countries (45 countries) and KCAL below 3088 (17 countries). 
4. The OECD does not appear to be a main decision node in our model, but there are 12 non-member countries with high obesity levels. 

## Conclusions
There are so many conclusions we can draw from a simple tree diagram. It does become a bit difficult difficult to present the information once we have so many features. Still, this was a great compliment to the findings from the previous post. In the future, I hope to use bigger databases to apply the predictive power of this Machine Learning tool. For now we have seen how to use it in Python and described the really complex relationships between our variables. Thanks for stopping by!
